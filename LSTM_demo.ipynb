{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OscarChang46/de-identify-sensitive-data-for-cybersecurity-use-cases-e.g.-PII-and-PHI-data-in-unstructured-text-an/blob/main/LSTM_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmYGdRlJgEsY",
        "outputId": "3e804e90-d8b4-43cc-c5a9-1b9562b66f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Xr_8JtMMgdyU"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YpZ4h62UgjEK"
      },
      "outputs": [],
      "source": [
        "# sample data\n",
        "def load_data():\n",
        "    # passengers number of international airline , 1949-01 ~ 1960-12 per month\n",
        "    seq_number = np.array(\n",
        "        [112., 118., 132., 129., 121., 135., 148., 148., 136., 119., 104.,\n",
        "         118., 115., 126., 141., 135., 125., 149., 170., 170., 158., 133.,\n",
        "         114., 140., 145., 150., 178., 163., 172., 178., 199., 199., 184.,\n",
        "         162., 146., 166., 171., 180., 193., 181., 183., 218., 230., 242.,\n",
        "         209., 191., 172., 194., 196., 196., 236., 235., 229., 243., 264.,\n",
        "         272., 237., 211., 180., 201., 204., 188., 235., 227., 234., 264.,\n",
        "         302., 293., 259., 229., 203., 229., 242., 233., 267., 269., 270.,\n",
        "         315., 364., 347., 312., 274., 237., 278., 284., 277., 317., 313.,\n",
        "         318., 374., 413., 405., 355., 306., 271., 306., 315., 301., 356.,\n",
        "         348., 355., 422., 465., 467., 404., 347., 305., 336., 340., 318.,\n",
        "         362., 348., 363., 435., 491., 505., 404., 359., 310., 337., 360.,\n",
        "         342., 406., 396., 420., 472., 548., 559., 463., 407., 362., 405.,\n",
        "         417., 391., 419., 461., 472., 535., 622., 606., 508., 461., 390.,\n",
        "         432.], dtype=np.float32)\n",
        "    # assert seq_number.shape == (144, )\n",
        "    # plt.plot(seq_number)\n",
        "    # plt.ion()\n",
        "    # plt.pause(1)\n",
        "    seq_number = seq_number[:, np.newaxis]\n",
        "\n",
        "    # print(repr(seq))\n",
        "    # 1949~1960, 12 years, 12*12==144 month\n",
        "    seq_year = np.arange(12)\n",
        "    seq_month = np.arange(12)\n",
        "    seq_year_month = np.transpose(\n",
        "        [np.repeat(seq_year, len(seq_month)),\n",
        "         np.tile(seq_month, len(seq_year))],\n",
        "    )  # Cartesian Product\n",
        "\n",
        "    seq = np.concatenate((seq_number, seq_year_month), axis=1)\n",
        "\n",
        "    # normalization\n",
        "    seq = (seq - seq.mean(axis=0)) / seq.std(axis=0)\n",
        "    return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.W_xh = nn.Linear(input_size, hidden_size)\n",
        "        self.W_hh = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    def __call__(self, x, hidden):\n",
        "        return self.step(x, hidden)\n",
        "\n",
        "    def step(self, x, hidden):\n",
        "        h1 = self.W_hh(hidden)\n",
        "        w1 = self.W_xh(x)\n",
        "        out = nn.Tanh(h1 + w1)\n",
        "        hidden = self.W_hh.weight\n",
        "        return out, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (W_xh): Linear(in_features=20, out_features=50, bias=True)\n",
              "  (W_hh): Linear(in_features=20, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rnn = RNN(20, 50)\n",
        "rnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0heOHzjphDpb"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-NFsiJSmhBNy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (gate): Linear(in_features=15, out_features=5, bias=True)\n",
              "  (output): Linear(in_features=5, out_features=10, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (tanh): Tanh()\n",
              "  (softmax): Softmax(dim=None)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.autograd import Variable\n",
        "import torch\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, cell_size):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.gate = nn.Linear(input_size + hidden_size, cell_size) \n",
        "    self.hidden_size = hidden_size\n",
        "    self.cell_size = cell_size\n",
        "    self.output = nn.Linear(hidden_size, input_size)\n",
        "    self.sigmoid =nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "\n",
        "  def forward(self, input, hidden, cell):\n",
        "    combined = torch.cat((input, hidden), 1)\n",
        "    f_gate = self.gate(combined)\n",
        "    f_gate = self.sigmoid(f_gate)\n",
        "    i_gate = self.gate(combined)\n",
        "    i_gate = self.sigmoid(i_gate)\n",
        "    o_gate = self.gate(combined)\n",
        "    o_gate = self.sigmoid(o_gate)\n",
        "    c_tilde = self.gate(combined)\n",
        "    c_tilde = self.tanh(c_tilde)\n",
        "    cell = torch.add(torch.mul(cell, f_gate) + torch.mul(c_tilde, i_gate))\n",
        "    hidden = torch.mul(self.tanh(cell, o_gate))\n",
        "    output = self.output(hidden)\n",
        "    output = self.softmax(output)\n",
        "\n",
        "    return output, hidden, cell\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return Variable(torch.zeros(1, self.hidden_size))\n",
        "\n",
        "  def initCell(self):\n",
        "    return Variable(torch.zeros(self.cell_size))\n",
        "\n",
        "input_dim = 10\n",
        "hidden_dim = 5\n",
        "cell_dim = 5\n",
        "output_dim = 10\n",
        "lstm = LSTM(input_dim, hidden_dim, cell_dim)\n",
        "# lstm\n",
        "seq_len = 108 # the length of sentence\n",
        "batch_size = 20# the number of sentence in the same batch\n",
        "input_dim = 3# dimension of vector representation\n",
        "input_data = torch.randn(108, 20, 3)\n",
        "lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_size = 64\n",
        "        drp = 0.1\n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
        "        self.linear = nn.Linear(self.hidden_size*4 , 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(drp)\n",
        "        self.out = nn.Linear(64, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_embedding = self.embedding(x)\n",
        "        h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))\n",
        "        \n",
        "        h_lstm, _ = self.lstm(h_embedding)\n",
        "        avg_pool = torch.mean(h_lstm, 1)\n",
        "        max_pool, _ = torch.max(h_lstm, 1)\n",
        "        #print(\"avg_pool\", avg_pool.size())\n",
        "        #print(\"max_pool\", max_pool.size())\n",
        "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
        "        conc = self.relu(self.linear(conc))\n",
        "        conc = self.dropout(conc)\n",
        "        out = self.out(conc)\n",
        "        return out\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conditional Random Field\n",
        "https://github.com/typoverflow/pytorch-crf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "from matplotlib.pyplot import polar\n",
        "import os\n",
        "from numpy.lib.npyio import load\n",
        "from feature_function import StatusFF, ObsFF, TransFF\n",
        "class CRF(nn.Module):\n",
        "    def __init__(self, d:LabelDict, ffshape):\n",
        "        super(CRF, self).__init__()\n",
        "        self.d = d\n",
        "        self.L = len(d)\n",
        "        self.K = 1\n",
        "        for d in ffshape:\n",
        "            self.K *= d\n",
        "        self.w = nn.Parameter(torch.zeros(self.K))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNQhDEF/yDdcws6qEajCr85",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "LSTM_demo.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8c9c99c14df453c5acfecca1ae41737996bc13b0c3a420c7c11ddf365101be87"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('tf_m1': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
